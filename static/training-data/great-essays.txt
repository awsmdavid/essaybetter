Atlantic Article
Buy Experiences, Not Things by James Hamblin

Diction Score
9.0

Complexity Variance
105.470374987
Average Sentence Length
18
Min Length
2
Max Length
62

Structural Score
0.666666666667

Certainty Score
0.497801507538

Forty-seven percent of the time, the average mind is wandering. It wanders about a third of the time while a person is reading, talking with other people, or taking care of children. It wanders 10 percent of the time, even, during sex. And that wandering, according to psychologist Matthew Killingsworth, is not good for well-being. A mind belongs in one place. During his training at Harvard, Killingsworth compiled those numbers and built a scientific case for every cliché about living in the moment. In a 2010 Science paper co-authored with psychology professor Daniel Gilbert, the two wrote that "a wandering mind is an unhappy mind."

For Killingsworth, happiness is in the content of moment-to-moment experiences. Nothing material is intrinsically valuable, except in whatever promise of happiness it carries. Satisfaction in owning a thing does not have to come during the moment it's acquired, of course. It can come as anticipation or nostalgic longing. Overall, though, the achievement of the human brain to contemplate events past and future at great, tedious length has, these psychologists believe, come at the expense of happiness. Minds tend to wander to dark, not whimsical, places. Unless that mind has something exciting to anticipate or sweet to remember.

Over the past decade, an abundance of psychology research has shown that experiences bring people more happiness than do possessions. The idea that experiential purchases are more satisfying than material purchases has long been the domain of Cornell psychology professor Thomas Gilovich. Since 2003, he has been trying to figure out exactly how and why experiential purchases are so much better than material purchases. In the journal Psychological Science last month, Gilovich and Killingsworth, along with Cornell doctoral candidate Amit Kumar, expanded on the current understanding that spending money on experiences "provide[s] more enduring happiness." They looked specifically at anticipation as a driver of that happiness; whether the benefit of spending money on an experience accrues before the purchase has been made, in addition to after. And, yes, it does.

Essentially, when you can't live in a moment, they say, it's best to live in anticipation of an experience. Experiential purchases like trips, concerts, movies, et cetera, tend to trump material purchases because the utility of buying anything really starts accruing before you buy it.


Mean self-reported ratings
(Kumar et al, Psychological Science/The Atlantic)
Waiting for an experience apparently elicits more happiness and excitement than waiting for a material good (and more "pleasantness" too—an eerie metric). By contrast, waiting for a possession is more likely fraught with impatience than anticipation. "You can think about waiting for a delicious meal at a nice restaurant or looking forward to a vacation," Kumar told me, "and how different that feels from waiting for, say, your pre-ordered iPhone to arrive. Or when the two-day shipping on Amazon Prime doesn’t seem fast enough."

Gilovich's prior work has shown that experiences tend to make people happier because they are less likely to measure the value of their experiences by comparing them to those of others. For example, Gilbert and company note in their new paper, many people are unsure if they would rather have a high salary that is lower than that of their peers, or a lower salary that is higher than that of their peers. With an experiential good like vacation, that dilemma doesn't hold. Would you rather have two weeks of vacation when your peers only get one? Or four weeks when your peers get eight? People choose four weeks with little hesitation.

Experiential purchases are also more associated with identity, connection, and social behavior. Looking back on purchases made, experiences make people happier than do possessions. It's kind of counter to the logic that if you pay for an experience, like a vacation, it will be over and gone; but if you buy a tangible thing, a couch, at least you'll have it for a long time. Actually most of us have a pretty intense capacity for tolerance, or hedonic adaptation, where we stop appreciating things to which we're constantly exposed. iPhones, clothes, couches, et cetera, just become background. They deteriorate or become obsolete. It's the fleetingness of experiential purchases that endears us to them. Either they're not around long enough to become imperfect, or they are imperfect, but our memories and stories of them get sweet with time. Even a bad experience becomes a good story.

When it rains through a beach vacation, as Kumar put it, "People will say, well, you know, we stayed in and we played board games and it was a great family bonding experience or something." Even if it was negative in the moment, it becomes positive after the fact. That's a lot harder to do with material purchases because they're right there in front of you. "When my Macbook has the colorful pinwheel show up," he said, "I can't say, well, at least my computer is malfunctioning!"

"At least my computer and I get to spend more time together because it's working so slowly," I offered.

"Yes, exactly."

"Maybe we should destroy our material possessions at their peak, so they will live on in an idealized state in our memories?"

"I don't know if I'd go that far," he said. "The possibility of making material purchases more experiential is sort of interesting."

That means making purchasing an experience, which is terrible marketing-speak, but in practical terms might mean buying something on a special occasion or on vacation or while wearing a truly unique hat. Or tying that purchase to subsequent social interaction. Buy this and you can talk about buying it, and people will talk about you because you have it.

"Turns out people don't like hearing about other people's possessions very much," Kumar said, "but they do like hearing about that time you saw Vampire Weekend."

I can't imagine ever wanting to hear about someone seeing Vampire Weekend, but I get the point. Reasonable people are just more likely to talk about their experiential purchases than their material purchases. It's a nidus for social connection. ("What did you do this weekend?" "Well! I'm so glad you asked ... ")

The most interesting part of the new research, to Kumar, was the part that "implies that there might be notable real-world consequences to this study." It involved analysis of news stories about people waiting in long lines to make a consumer transaction. Those waiting for experiences were in better moods than those waiting for material goods. "You read these stories about people rioting, pepper-spraying, treating each other badly when they have to wait," he said. It turns out, those sorts of stories are much more likely to occur when people are waiting to acquire a possession than an experience. When people are waiting to get concert tickets or in line at a new food truck, their moods tend to be much more positive.

"There are actually instances of positivity when people are waiting for experiences," Kumar said, like talking to other people in the concert line about what songs Vampire Weekend might play. So there is opportunity to connect with other people. "We know that social interaction is one of the most important determinants of human happiness, so if people are talking with each other, being nice to one another in the line, it's going to be a lot more pleasant experience than if they're being mean to each other which is what's (more) likely to happen when people are waiting for material goods."

Research has also found that people tend to be more generous to others when they've just thought about an experiential purchase as opposed to a material purchase. They're also more likely to pursue social activities. So, buying those plane tickets is good for society. (Of course, maximal good to society and personal happiness comes from pursuing not happiness but meaning. All of this behavioral economics-happiness research probably assumes you've already given away 99 percent of your income to things bigger than yourself, and there's just a very modest amount left to maximally utilize.)

What is it about the nature of imagining experiential purchases that's different from thinking about future material purchases? The most interesting hypothesis is that you can imagine all sort of possibilities for what an experience is going to be. "That's what's fun," Kumar said. "It could turn out a whole host of ways." With a material possession, you kind of know what you're going to get. Instead of whetting your appetite by imagining various outcomes, Kumar put it, people sort of think, Just give it to me now.

It could turn out that to get the maximum utility out of an experiential purchase, it's really best to plan far in advance. Savoring future consumption for days, weeks, years only makes the experience more valuable. It definitely trumps impulse buying, where that anticipation is completely squandered. (Never impulse-buy anything ever.)

That sort of benefit would likely be a lot stronger in an optimistic person as opposed to a pessimistic person. Some people hate surprises. Some people don't anticipate experiences because they dwell on what could—no, will—go wrong. But we needn't dwell in their heads. Everyone can decide on the right mix of material and experiential consumption to maximize their well-being. The broader implications, according to Gilovich in a press statement, are that "well-being can be advanced by providing infrastructure that affords experiences, such as parks, trails, and beaches, as much as it does material consumption." Or at least the promise of that infrastructure, so we can all look forward to using it. And when our minds wander, that's where they'll go.





GMAT ESSAY

Complexity Grade
Great
Complexity Variance
42.4028300942
Average Sentence Length
29
Max Length
46
Min Length
5

Structural Score
0.692307692308

Certainty Score
0.495133819951

In light of the disturbing increase in crime in our cities and suburbs in recent years, many Americans have expressed an interest in the use of flogging to punish young offenders guilty of crimes such as vandalism. I am opposed to the use of flogging as punishment based on humanitarian, psychological, and moral grounds. 

While I share the frustration of other law-abiding citizens who are trying to stem the tide of senseless destruction of personal property, I believe that flogging is a cruel and uncivilized form of punishment that has no place in our country. The public infliction of painful physical punishment was banished from civilized countries years ago with the disappearance of the stocks, pillories, and public whippings. As witnessed in the case of the American teenager who was sentenced to a caning for vandalism in Singapore, most civilized countries around the world objected vehemently to the severity and barbarism of this form of punishment for a nonviolent crime. 

On psychological grounds, inflicting physical punishment to teach a lesson has been frowned upon for years by child psychologists and behavioral experts. Spankings, beltings, and beatings are all considered forms of abuse and have been proved to have only very negative affects on behavior. Studies have also supported the conclusion that violence begets violence, and it would seem very probable that the use of flogging to punish a young person who committed vandalism may well lead to a more violent expression of anger next time. The troubled individual whose antisocial behavior was directed toward property may well be incited to take his anger out in a physically violent way against people after being subjected to such treatment. 

Finally, on moral grounds, we need to make the distinction between crimes against persons and crimes against property. In terms of the Biblical injunction of "an eye for an eye, a tooth for a tooth," inflicting physical injury on an offender who committed damage to property is hardly equitable. What is the lesson we are trying to teach? In the 1970s, the movie "A Clockwork Orange" dealt with the issue of violent crime in a futuristic setting and society's increasingly cruel methods of "rehabilitation." It left the viewer questioning which was more barbaric, the crime or the punishment? 

In conclusion, we must continue to search for ways to reduce the incidence of crimes against property as well as persons, but we must above all keep sight of our humanity. Flogging is not the answer. 





###
The Atlantic
The Real Roots of Midlife Crisis
Diction Score
32.0

Complexity Grade
Incredible
Complexity Variance
245.089779469
Average Sentence Length
22
Max Length
79
Min Length
1

Structural Score
0.722772277228


Certainty Score
0.4968232737

This summer, a friend called in a state of unhappy perplexity. At age 47, after years of struggling to find security in academia, he had received tenure. Instead of feeling satisfied, however, he felt trapped. He fantasized about escape. His reaction had taken him by surprise. It made no sense. Was there something wrong with him? I gave him the best answer I know. I told him about the U-curve.

Not everyone goes through the U-curve. But many people do, and I did. In my 40s, I experienced a lot of success, objectively speaking. I was in a stable and happy relationship; I was healthy; I was financially secure, with a good career and marvelous colleagues; I published a book, wrote for top outlets, won a big journalism prize. If you had described my own career to me as someone else’s, or for that matter if you had offered it to me when I was just out of college, I would have said, “Wow, I want that!” Yet morning after morning (mornings were the worst), I would wake up feeling disappointed, my head buzzing with obsessive thoughts about my failures. I had accomplished too little professionally, had let life pass me by, needed some nameless kind of change or escape.

My dissatisfaction was whiny and irrational, as I well knew, so I kept it to myself. When I thought about it—which I did, a lot—I rejected the term midlife crisis, because I was holding a steady course and never in fact experienced a crisis: more like a constant drizzle of disappointment. What annoyed me most of all, much more than the disappointment itself, was that I felt ungrateful, the last thing in the world I was entitled to be. Hopeful that rationality might prevail, I would count my blessings, quite literally—making lists mentally, and sometimes also on paper of all that I had to be thankful for. Reasoning with myself might help for a little while, but then the disappointment would return. As the weeks turned into months, and then into years, my image of myself began to change. I had always thought of myself as a basically happy person, but now I seemed to be someone who dwelt on discontents, real or imaginary. I supposed I would have to reconcile myself to being a malcontent.

As I moved into my early 50s, I hit some real setbacks. Both of my parents died, one of them after suffering a terrible illness while I watched helplessly. My job disappeared when the magazine I worked for was restructured. An entrepreneurial effort—to create a new online marketplace that would match journalists who had story ideas with editors looking for them—ran into problems. My shoulders, elbows, and knees all started aching. And yet the fog of disappointment and self-censure began to lift, at first almost imperceptibly, then more distinctly. By now, at 54, I feel as if I have emerged from a passage through something. But what?

Long ago, when I was 30 and he was 66, the late Donald Richie, the greatest writer I have known, told me: “Midlife crisis begins sometime in your 40s, when you look at your life and think, Is this all? And it ends about 10 years later, when you look at your life again and think, Actually, this is pretty good.” In my 50s, thinking back, his words strike me as exactly right. To no one’s surprise as much as my own, I have begun to feel again the sense of adventure that I recall from my 20s and 30s. I wake up thinking about the day ahead rather than the five decades past. Gratitude has returned.

I was about 50 when I discovered the U-curve and began poking through the growing research on it. What I wish I had known in my 40s (or, even better, in my late 30s) is that happiness may be affected by age, and the hard part in middle age, whether you call it a midlife crisis or something else, is for many people a transition to something much better—something, there is reason to hope, like wisdom. I wish someone had told me what I was able to tell my worried friend: nothing was wrong with him, and he wasn’t alone.

In the 1970s, an economist named Richard Easterlin, then at the University of Pennsylvania, learned of surveys gauging people’s happiness in countries around the world. Intrigued, he set about amassing and analyzing the data, in the process discovering what came to be known as the Easterlin paradox: beyond a certain point, countries don’t get happier as they get richer. Today he is at the University of Southern California and is celebrated as the founder of a new branch of economics, focused on human well-being. At the time, though, looking at something as subjective as happiness seemed eccentric to mainstream economists. His findings, Easterlin says, were for many years regarded as a curiosity, more a subject for cocktail conversation than for serious research.

A generation later, in the 1990s, happiness economics resurfaced. This time a cluster of labor economists, among them David Blanchflower of Dartmouth and Andrew Oswald of the University of Warwick, got interested in the relationship between work and happiness. That led them to international surveys of life satisfaction and the discovery, quite unexpected, of a recurrent pattern in countries around the world. “Whatever sets of data you looked at,” Blanchflower told me in a recent interview, “you got the same things”: life satisfaction would decline with age for the first couple of decades of adulthood, bottom out somewhere in the 40s or early 50s, and then, until the very last years, increase with age, often (though not always) reaching a higher level than in young adulthood. The pattern came to be known as the happiness U-curve.

Meanwhile, Carol Graham, a development economist (she is now at the Brookings Institution, where I’m a senior fellow), was looking at Peruvians who had emerged rapidly from poverty. “How do these people think they’ve done?” she wanted to know. She told me she was startled to find that objective life circumstances did not determine subjective life satisfaction; in Peru, as in other countries, many people who had moved out of poverty felt worse off than those who had stayed poor. “I didn’t know how to explain it,” she said. Hunting around, she discovered the sparse literature on the economics of happiness, plunged into survey data, and found the same U-shaped pattern, first in Latin America and then in the rest of the world. “It was a statistical regularity,” she said. “Something about the human condition.”


The U-curve emerges in answers to survey questions that measure satisfaction with life as a whole, not mood from moment to moment. The exact shape of the curve, and the age when it bottoms out, vary by country, survey question, survey population, and method of statistical analysis. The U-curve is not ubiquitous; indeed, one would be suspicious if a single pattern turned up across an immensely variegated landscape of surveys and countries and generations and analyses. Still, the pattern turns up much too often to ignore. For example, in a 2008 study, Blanchflower and Oswald found the U-curve—with the nadir, on average, at age 46—in 55 of 80 countries where people were asked, “All things considered, how satisfied are you with your life as a whole these days?” Graham and Milena Nikolova recently looked at an international survey that asked people in 149 countries to rate their lives on a zero-to-10 scale where 10 “represents the best possible life for you” and zero the worst. They found a relationship between age and happiness in 80 countries, and in all but nine of those, satisfaction bottomed out between the ages of 39 and 57 (the average nadir was at about age 50).

The curve tends to evince itself more in wealthier countries, where people live longer and enjoy better health in old age. Sometimes it turns up directly in raw survey data—that is, people just express less overall satisfaction in middle age. But here’s a wrinkle: in many cases (including the two analyses I just cited), the age-based U-curve emerges only after researchers adjust for such variables as income, marital status, employment, and so on, thus looking through to the effects of age alone. Some scholars—including Easterlin, the grand old man of the field—take a dim view of making such adjustments. Carol Ryff, a psychologist who directs the University of Wisconsin’s Institute on Aging, told me, “To my mind, that’s how you obscure the story; that’s not how you clean it up.” But filtering out important life circumstances suggests something intriguing: there may be an underlying pattern in life satisfaction that is independent of your situation. In other words, if all else is equal, it may be more difficult to feel satisfied with your life in middle age than at other times. Blanchflower and Oswald have found that, statistically speaking, going from age 20 to age 45 entails a loss of happiness equivalent to one-third the effect of involuntary unemployment.

“I view this as a first-order discovery about human beings that will outlive us by hundreds of years,” Oswald told me. Not everyone is prepared to go so far. Many psychologists have their doubts, partly because the U-curve is a statistical regularity that emerges from large data sets, and psychologists prefer to study actual people, whether individually or in experimental groups, and ideally across their whole lives. “I think it’s a mistake to generalize about life-course patterns,” Ryff told me. “In the final analysis, you’re not talking about real people when you tell these big, generic stories.” Heretofore, when psychologists have gone looking for evidence of midlife crisis—that is, of a distinctive phenomenon of middle age, rather than just stress or difficulty that might come at any point in life—they haven’t found it, and they are cool to the possibility that the smoking gun has turned up in economics, of all places.

In recent work, however, U-curve researchers have begun to find evidence that is harder to dismiss as mere statistical correlation. Oswald, Terence Cheng, and Nattavudh Powdthavee have found the U-curve in four longitudinal data sets from three countries: an important kind of evidence, because it traces the lived experiences of individuals over time, rather than comparing people of various ages in a statistical snapshot. Likewise, Blanchflower and Oswald, looking at samples from 27 European countries, have found a “strong hill-shaped pattern” in the use of antidepressants, peaking in people’s late 40s. Being middle-aged “nearly doubles” a person’s likelihood of using antidepressants. The same pattern appears, they’ve found, in the two U.S. states that collect the relevant data (New Hampshire and New Mexico).

And a lot of eyebrows went up when Oswald and four other scholars, including two primatologists, found a U-shaped curve in chimpanzees’ and orangutans’ state of mind over time. Zookeepers, researchers, and other animal caretakers filled out a questionnaire rating the well-being of their primate charges (more than 500 captive chimps and orangutans in Australia, Canada, Japan, Singapore, and the United States). The apes’ well-being bottomed out at ages comparable, in people, to between 45 and 50. “Our results,” the authors concluded in a 2012 paper, “imply that human wellbeing’s curved shape is not uniquely human and that, although it may be partly explained by aspects of human life and society, its origins may lie partly in the biology we share with closely related great apes.”

I think where the evidence points is this: being satisfied is perfectly possible in midlife, but for a great many of us it is harder. That is how the U-curve felt to me, and how it feels to some of the people I unscientifically surveyed for this article.

“I think it must be something internal,” my 45-year-old friend S. told me. He described his 20s as exciting and fun (“I was really dumb but thought I knew a lot”) and his 30s as a time of hard work and steady rewards (“I felt on track … Things looked like white picket fences and the American dream”), but said he was bushwhacked in his 40s by an unexpected divorce, unmarried fatherhood, and a heart attack. He said he now experiences difficulty feeling contentment, leading to some of the same self-doubt that I felt: a creeping suspicion that he is fated to be whiny. He also wondered whether his dissatisfaction has been a cause of some of his problems, not just an effect. “Professionally, things looked pretty good,” S. told me. “But maybe something was going on. Something sufficient for my wife to leave. If I did a deep psychological dive, I might say that nothing will ever make me content. Maybe there’s something deeply psychologically wrong with me. I see life as a challenge to overcome rather than an adventure to be enjoyed. I’ve thought of running away to Brazil—changing my name and becoming a hotel clerk. Maybe that will change in my 50s.”

I was happy to tell him that the odds are in his favor.


My friend K. is a 54-year-old woman whose trajectory somewhat resembles S.’s. She had an exciting launch in her 20s (working “my dream job”), a sense of continuing achievement but slowing momentum in her 30s (“sort of a slog”), and an ambush in her 40s, when her father died, her mother had a stroke, her husband left her after their daughter was born, and she was laid off. Despite coping with all of that and doing well professionally, even with her layoff (“I did better in my career; I made more money”), she developed what she describes as a dark sense of humor about her life, ruefully telling herself that at least she had so many troubles that she couldn’t dwell on all of them at once.

In the past few years, things have turned upward, markedly so. K.’s 50s have brought not only less external turbulence but less on the inside, too. “On a day-to-day basis, I probably do the same things, but I feel different,” she says. Her values have shifted away from work: “I could see that was not going to be a big source of achievement. I measure my worth now by how I can help others and contribute to the community. I enjoy the relationships that I’ve been able to nurture over these years—longer-term friends, growing with those friends. It was always striving and looking ahead, as opposed to being in the now and feeling grateful for the now. I think I feel a great gratitude. When I am in a situation when I can moan a little bit or feel bad about some of the difficult things that have happened, the balance sheet is hugely on the side of all the great things that have happened. And I think that gratitude has helped me be both more satisfied and more giving.” She describes her 50s, so far, as good and improving.

The same has been true for me. Though I still have my share of gloomy days, I find it far easier than I did in my 40s to appreciate what I have, even without writing down lists of good things, as I had to resort to doing a decade ago. It certainly helps that my pet cause, gay marriage, has met with success, and that I myself achieved legal marriage at age 50. But something has changed inside, too, because in my 40s, I had plenty of success and none of it seemed adequate, which was why I felt so churlish. For me, after a period when gratitude seemed to have abandoned me, its return feels like a gift.

It turns out that there is good science about this gift: studies show quite strongly that people’s satisfaction with their life increases, on average, from their early 50s on through their 60s and 70s and even beyond—for many until disability and final illness exact their toll toward the very end (at which point it’s hard to generalize). In a 2011 study, for example, the Stanford University psychologist Laura Carstensen and seven colleagues found that “the peak of emotional life may not occur until well into the seventh decade”—a finding that is “often met with disbelief in both the general population and the research community,” despite its strength. Carstensen described to me this pattern in her own life. “Forties were, for me, the worst,” she said. “You’re never good enough professionally. I think you’re coming out of the fog in the 50s.” Now, at 60, she said, “I feel so privileged. I feel it now.” Elaine Wethington, a professor of human development and sociology at Cornell, whose research likewise finds that people become more satisfied and more optimistic later in life, is in her early 60s and reports her own turning point at about age 50. “I feel like I’ve reached a kind of flow in my work and career,” she told me. No guarantees, of course, but reporting this article has led me to think that the upturn I’ve felt in my 50s is likely to continue. As Andrew Oswald exclaimed to me when I mentioned my own post-40s upswing: “Just wait until you’re 60!”

Of course, the most interesting question, and unfortunately also the hardest question, is: Why is happiness so often U-shaped? Why the common dissatisfaction in middle age? And why the upswing afterward?

Part of the answer likely involves what researchers call selection bias: unhappier people tend to die sooner, removing themselves from the sample. Also, of course, middle age is often a stressful time, burdened with simultaneous demands from jobs, kids, and aging parents. Those explanations don’t seem adequate by themselves, though. I can attest that I experienced the U-curve without dying off in the process; so do other people, as we know from happiness research that follows individuals over time. And recall that the U-curve often emerges after adjusting for other variables in life (children, income, job, marriage), so it is not purely situational.

A common hypothesis, and one that seems right to me, is alluded to by Carstensen and her colleagues in their 2011 paper: “As people age and time horizons grow shorter,” they write, “people invest in what is most important, typically meaningful relationships, and derive increasingly greater satisfaction from these investments.” Midlife is, for many people, a time of recalibration, when they begin to evaluate their lives less in terms of social competition and more in terms of social connectedness. In my 40s, I found I was obsessively comparing my life with other people’s: scoring and judging myself, and counting up the ways in which I had fallen behind in a race. Where was my best seller? My literary masterpiece? Barack Obama was younger than I, and look where he was! In my 50s, like my friend K., I find myself more inclined to prize and enjoy people and relationships, which mercifully seem to be pushing the unwinnable status competition into the background. Also, Carstensen told me, “when the future becomes less distant, more constrained, people focus on the present, and we think that’s better for emotional experience. The goals that are chronically activated in old age are ones about meaning and savoring and living for the moment.” These are exactly the changes that K. and others in my own informal research sample reported.

In my own case, however, what seems most relevant is a change frequently described both in popular lore and in the research literature: for some reason, I became more accepting of my limitations. “Goals, because they’re set in temporal context, change systematically with age,” Carstensen says. “As people perceive the future as increasingly constrained, they set goals that are more realistic and easy to pursue.” For me, the expectation of scaling ever greater heights has faded, and with it my sense of disappointment and failure.

The idea that the expectations gap closes with age has recently received some empirical backing, in the form of fascinating findings by Hannes Schwandt, a young economist at Princeton University’s Center for Health and Wellbeing. He used a German longitudinal survey, with data from 1991 to 2004, that, unusually, asked people about both their current life satisfaction and their expected satisfaction five years hence. That allowed him to compare expectations with subsequent reality for the same individuals over time. To his own surprise, he found the same result regardless of respondents’ economic status, generation, and even whether they lived in western or eastern Germany (two very different cultures): younger people consistently and markedly overestimated how satisfied they would be five years later, while older people underestimated future satisfaction. So youth is a period of perpetual disappointment, and older adulthood is a period of pleasant surprise. What’s more, Schwandt found that in between those two periods, during middle age, people experienced a sort of double whammy: satisfaction with life was declining (that’s the U-curve, which manifested itself clearly), but expectations were also by then declining (in fact, they were declining even faster than satisfaction itself). In other words, middle-aged people tend to feel both disappointed and pessimistic, a recipe for misery. Eventually, however, expectations stop declining. They settle at a lower level than in youth, and reality begins exceeding them. Surprises turn predominantly positive, and life satisfaction swings upward. And the crossover, in Schwandt’s sample, happened about where you would expect: in the 50s.

“This finding,” Schwandt writes, “supports the hypothesis that the age U-shape in life satisfaction is driven by unmet aspirations that are painfully felt during midlife but beneficially abandoned and felt with less regret during old age.”

Okay, but why does this abandonment and reorientation seem to happen so reliably in midlife? Firm explanations are some years away. Still, clues have emerged from the realm of brain science, and they hint at an answer that is both heartening and ancient.

Dilip V. Jeste is a distinguished psychiatrist with an unusual pedigree. On the “distinguished” side of the ledger are his multiple professorial titles at the University of California at San Diego, his recent presidency of the American Psychiatric Association, and his record as one of the country’s most prolific geriatric psychiatrists. Awards and certifications take up an entire wall of his office and range from the American College of Psychiatrists’ Award for Research in Geriatric Psychiatry to San Diego Magazine’s “Top Doctors 2013.” On the “unusual” side is his upbringing in a small town in India (he speaks with a lilting Marathi accent), his decision to focus his medical career on helping the elderly age successfully rather than on merely treating their ailments—and his belief that wisdom is a concept that belongs not just to Aesop and Aristotle but to cutting-edge neuroscience. Jeste, who is 70 and thin enough to look frail until you notice his nimble gait, is no mystic. He and his colleagues use magnetic-scanning technology and batteries of psychological tests to peer into the brain for clues to how the mind and emotions work.

As a teenager in India, Jeste came across Freud’s The Interpretation of Dreams. “It was like an Agatha Christie mystery,” he says, and it put him on the path to a degree in psychiatry and a research career in the United States, first at the National Institutes of Health and then at UCSD. Studying elderly schizophrenics, he was startled to find that they did better as they aged. That led him to explore how people can age successfully—that is, happily—despite health problems and other adverse circumstances. In 2006, and again in 2013, he published findings that people feel better, not worse, about their lives as they move through their later decades, even with the onset of chronic health problems that would lead one to expect distress or depression. “It was really a big surprise,” he says.

“The question I asked,” Jeste told me, “was, ‘Is there anything cognitive that actually improves with aging?’ That led me to think about wisdom. I started wondering whether the life satisfaction we were seeing in older people was related to their becoming wiser with age, in spite of physical disability.”

His medical colleagues were skeptical, to say the least, telling him that the study of wisdom should be left to philosophers, not neuroscientists or psychiatrists. “I took that as a challenge.” Jeste grew up in a world steeped in reverence for wisdom. “It’s cultural, growing up in India,” he said. “We read the Gita, the Hindu Bible, as it were. But the Gita really is a document about what a wise person should do.” In recent years, Western psychiatry has generated a very small but scientifically legitimate body of research literature on wisdom. Surveying both modern research and ancient texts, Jeste found that the concept of wisdom has stayed “surprisingly similar” across centuries and across geographic regions: “All across the world, we have an implicit notion of what a wise person is.” The traits of the wise tend to include compassion and empathy, good social reasoning and decision making, equanimity, tolerance of divergent values, comfort with uncertainty and ambiguity. And the whole package is more than the sum of the parts, because these traits work together to improve life not only for the wise but also for their communities. Wisdom is pro-social. (Has any society ever wanted less of it?) Humans, Jeste says, live for an unusually long time after their fertile years; perhaps wisdom provides benefits to our children or our social groups that make older people worth keeping around, from an evolutionary perspective.

“Wisdom is useful at any age,” he says. “But from an evolutionary point of view, younger people are fertile, so even if they’re not wise, they’re okay. But older people need to find some other way that they can contribute to the survival of the species.” It’s also possible, he says, that “competitiveness may be more favored in the young, and more emotional regulation, more tolerance of diversity, more insight, in older people.” In any case, the very universality of the concept of wisdom, Jeste believes, suggests some biological basis. Which is why he’s seeking wisdom’s roots in the brain.

In San Diego this summer, I watched as Jeste and Lisa Eyler, a clinical psychologist at UCSD, conducted brain-imaging experiments to learn how older people process tasks related to compassion—an element of wisdom. They greeted J., a 71-year-old business coach, and outfitted her with earplugs, a head-mounted optical device that let her see projected images, a “button box” that let her respond to what she saw, and a panic button to stop the experiment. Then she was swallowed by a massive and impressively noisy functional-MRI scanning machine. She spent an hour performing tasks designed to stimulate both cognitive and emotional centers—remembering letters, matching facial expressions—while computers recorded images of her brain at work. This was followed by half an hour in front of a laptop as a postdoctoral researcher conducted a standardized empathy test, showing J. photos—some images benign, some upsetting—and recording her reactions. Finally came an interview with a clinician. Did J. consider herself a wise person? Sometimes—more so when she had time to reflect than when she was in a crisis. Had her wisdom increased with age? Yes, definitely. Had that made a difference? Yes again; she had learned not to act so precipitously, and she was better at seeing the best in others, even if it didn’t show on the surface. All of this information would be collated with results from dozens of other subjects and combed for insights into the neurology of compassion in older people; those results, in turn, will add a tile to the wisdom mosaic.

The science of wisdom is in its infancy, and as of now there is no evidence, Jeste says, that people get wiser as a result of aging per se (as opposed to learning from experience over time—also, of course, an element of wisdom). And there is no “wisdom organ” in the brain. Wisdom is an inherently multifarious trait, an emergent property of many other functions. (A psychological screening test for wisdom contains 39 quite diverse questions, although psychologists at UCSD are working on reducing the number to a more manageable dozen or so.)

But it does look likely that some elements of aging are conducive to wisdom, and to greater life satisfaction. In a 2012 paper evocatively titled “Don’t Look Back in Anger! Responsiveness to Missed Chances in Successful and Nonsuccessful Aging,” a group of German neuroscientists, using brain scans and other physical tests of mental and emotional activity, found that healthy older people (average age: 66) have “a reduced regret responsiveness” compared with younger people (average age: 25). That is, older people are less prone to feel unhappy about things they can’t change—an attitude consistent, of course, with ancient traditions that see stoicism and calm as part of wisdom. In fact, it is well established that older people’s brains react less strongly to negative stimuli than younger people’s brains do. “Young people just have more negative feelings,” Elaine Wethington, the Cornell professor, told me. Older brains may thus be less susceptible to the furies that buffet us earlier in life. Also, as Laura Carstensen, the Stanford psychologist, told me (summarizing a good deal of evidence), “Young people are miserable at regulating their emotions.” Years ago, my father made much the same point when I asked him why in his 50s he stopped having rages, which had shadowed his younger years and disrupted our family: “I realized I didn’t need to have five-dollar reactions to nickel provocations.”

Other studies find that social reasoning and long-term decision making improve with age; that spirituality increases (especially among women); that older adults feel more comfortable coping with uncertainty and ambiguity. Particularly intriguing are findings by Jeste and his colleagues suggesting that older people compensate for deterioration in specific regions of the brain by recruiting additional neural networks in other regions—an increase in so-called neuroplasticity that compensates for cognitive decline and perhaps brings other benefits. Jeste also notes that the brain circuits linked to rewards lose some sensitivity with age, possibly reducing impulsivity and addictive tendencies.

None of this, again, proves that people automatically get wiser with age (or more satisfied, or more calm, or more grateful). Many young people are wise, and many old people are not. It does hint, however, that aging changes us in ways that make it easier to be wise (and satisfied, and calm, and grateful). And I believe it suggests the need to rethink the meaning of midlife.

In the 1990s and early 2000s, when David Blanchflower and Andrew Oswald and Carol Graham and others began investigating the U-curve, almost no one seemed interested. And now? “It’s ridiculous,” Graham told me. “I can’t keep up with it.” In a few years, science will know a great deal more about the relationship between aging and life satisfaction, and it may even be able to apply some of that knowledge in ways that help us get through the hard patches and be, or become, wise. I believe, though, that the larger significance of the U-curve is not scientific or medical at all, but cultural. The U-curve offers an opportunity for society to tell a different and better story about life in middle age and beyond: a story that is more accurate and more forgiving and much less embarrassing and lonely.

The dominant story now, of course, is the narrative of midlife crisis. Although the idea of middle age as a distinct time of life dates back to the 19th century (according to Patricia Cohen, the author of In Our Prime: The Invention of Middle Age), the idea of a midlife crisis as such is quite recent, first appearing in 1965, in an article by the late psychologist Elliott Jaques. In 1974, in her best-selling book Passages: Predictable Crises of Adult Life, Gail Sheehy depicted midlife crisis with the example of a 40-year-old man who

has reached his professional goal but feels depressed and unappreciated. He blames his job or his wife or his physical surroundings for imprisoning him in this rut. Fantasies of breaking out begin to dominate his thoughts. An interesting woman he has met, another field of work, an Elysian part of the country—any or all of these become magnets for his wishes of deliverance. But once these objects of desire become accessible, the picture often begins to reverse itself. The new situation appears to be the dangerous trap from which he longs to take flight by returning to his old home base and the wife and children whose loss suddenly makes them dear.
No wonder many wives stand aghast.
This is not a bad description of how I felt in my 40s. All praise to Sheehy for her insight. Note, however, the element of disapproval that creeps in as “wives stand aghast.” Society stands aghast, too. Almost as soon as it was born, the social narrative of midlife crisis took on connotations of irresponsibility, escapism, self-indulgence, antisocial behavior. Wethington, the Cornell psychologist, found in research she published in 2000 that about a quarter of Americans reported experiencing a midlife crisis, and that many who disclaimed the notion regarded midlife crisis as a lame excuse for behaving immaturely. The term crisis also contributes to the stigma, because it suggests a shock or disruption or loss of control, when the evidence points to something much more like an extended and unpleasant but manageable downturn.

The story of the U-curve, I think, tells an emotionally fairer and more accurate tale. It is a story not of chaos or disruption but of a difficult yet natural transition to a new equilibrium. And I find that when I tell troubled middle-aged people about it, their reaction is one of relief. Just knowing that the phenomenon is common can be therapeutic. Hannes Schwandt, of Princeton, notes what he calls a feedback effect: “Part of your disappointment is driven by the disappointment itself.” If more people understood how common the U-shaped pattern is, they might be less inclined to make the forecasting errors that contribute to disappointment—and also less inclined to judge themselves harshly for feeling disappointed.

“When I give lectures, I say we’re stuck with this,” Andrew Oswald told me, “but at least you know it’s completely normal if you’re feeling low in your 40s.” He adds: “And when you’re low, you blame the wrong things.” People thrash around for explanations, which can lead to attribution errors and bad decisions. And those, of course, can bring on what really is a stereotypical midlife crisis, complete with lurching change and ill-judged behavior. In my late 40s, my own nameless dissatisfaction, like a parasitic wasp searching for a host, fixed upon my career and pestered me with an unbidden and unwelcome but insistent urge to quit my magazine column—today, right now, what was I waiting for? Fortunately my better judgment and my friends stopped me from acting on what would have been a useless and self-destructive whim. Still, in hindsight, I wish I had been forewarned that the U-curve, not my column, was the likely source of my discontent, and that a lot of other people, and possibly also a lot of other primates, were in the same boat.

Science has a great deal to learn about the intersection of aging and happiness, but I don’t think it is too early to begin spreading the word about the U-curve. And so I tell people in their 30s and 40s that nothing is written in stone, and that they may sail through midlife in grand emotional style—but if not, they aren’t alone, and usually it gets better, so march through it and don’t do anything stupid. When George Orwell was 40 (he died at only 46), he wrote: “Any life when viewed from the inside is simply a series of defeats.” He was wrong, thank goodness—as perhaps I am gaining the wisdom to see.



Attitude - Margaret Atwood
http://www.humanity.org/voices/commencements/margaret-atwood-university-toronto-speech-1983

Diction Score
8.0

Complexity Grade
Incredible
Complexity Variance
179.52715672
Average Sentence Length
27
Max Length
115
Min Length
1

Structural Score
0.933333333333

Certainty Score
0.496395963479

I am of course overjoyed to be here today in the role of ceremonial object. There is more than the usual amount of satisfaction in receiving an honorary degree from the university that helped to form one’s erstwhile callow and ignorant mind into the thing of dubious splendor that it is today; whose professors put up with so many overdue term papers, and struggled to read one’s handwriting, of which ‘interesting’ is the best that has been said; at which one failed to learn Anglo-Saxon and somehow missed Bibliography entirely, a severe error which I trust no one present here today has committed; and at which one underwent excruciating agonies not only of soul but of body, later traced to having drunk too much coffee in the bowels of Wymilwood.

It is to Victoria College that I can attribute the fact that Bell Canada, Oxford University Press and McClelland and Stewart all failed to hire me in the summer of ‘63, on the grounds that I was a) overqualified and b) couldn’t type, thus producing in me that state of joblessness, angst and cosmic depression which everyone knows is indispensable for novelists and poets, although nobody has ever claimed the same for geologists, dentists or chartered accountants. It is also due to Victoria College, incarnated in the person of Northrop Frye, that I didn’t run away to England to become a waitress, live in a garret, write masterpieces and get tuberculosis. He thought I might have more spare time for creation if I ran away to Boston, lived in a stupor, wrote footnotes and got anxiety attacks, that is, if I went to Graduate School, and he was right. So, for all the benefits conferred upon me by my Alma Mater, where they taught me that the truth would make me free but failed to warn me of the kind of trouble I’d get into by trying to tell it - I remain duly grateful.

But everything has its price. No sooner had I tossed off a graceful reply to the letter inviting me to be present today than I began to realize the exorbitance of what was expected of me. I was going to have to come up with something to say, to a graduating class in 1983, year of the Ph.D. taxi driver, when young people have unemployment the way they used to have ugly blackheads; something presumably useful, wise, filled with resonance and overview, helpful, encouraging and optimistic. After all, you are being launched - though ever since I experienced the process, I’ve wondered why “convocation” is the name for it. “Ejection” would be better. Even in the best of times, it’s more or less like being pushed over a cliff, and these are not the best of times. In case you haven’t figured it out already, I’m here to tell you that it’s an armpit out there. As for your university degree, there are definitely going to be days when you will feel that you’ve been given a refrigerator and sent to the middle of a jungle, where there are no three-pronged grounded plugholes.

Not only that, the year will come when you will wake up in the middle of the night and realize that the people you went to school with are in positions of power, and may soon actually be running things. If there’s anything more calculated to thick men’s blood with cold, it’s that. After all, you know how much they didn’t know then, and, given yourself as an example, you can’t assume they know a great deal more now. “We’re all doomed,” you will think. (For example: Brian Mulroney is only a year older than I am.) You may feel that the only thing to do when you’ve reached this stage is to take up nail-biting, mantras, or jogging, all of which would be recognized by animal behavior specialists as substitution activities, like scratching, which are resorted to in moments of unresolved conflict. But we’ll get around to some positive thinking in a moment.

“What shall I tell them!” I thought, breaking out into a cold sweat, as I tossed and turned night after night. (Lest you leap to indulge in Calvinistic guilt at the idea of having been the proximate cause of my discomfort, let me hasten to add that I was on a boat. The tossing and turning was par for the course, and the cold sweat can be cured by Gravol). For a while I toyed with the idea of paraphrasing Kurt Vonnegut, who told one graduating class, “Everything is going to become unbelievably worse and will never get better again,” and walked off the stage. But that’s the American style: boom or bust. A Canadian would be more apt to say, “things may be pretty mediocre but let’s at least try to hold the line.”

Then I thought that maybe I could say a few words on the subject of a liberal arts education, and how it prepares you for life. But sober reflection led me to the conclusion that this topic too was a washout; for, as you will soon discover, a liberal arts education doesn’t exactly prepare you for life. A preparation-for-life curriculum would not consist of courses on Victorian Thought and French Romanticism, but of things like How to Cope With Marital Breakdown, Getting More for your Footwear Dollar, Dealing With Stress, and How To Keep Your Fingernails from Breaking Off by Always Filing Them Towards the Center; in other words, it would read like the contents page of Homemakers Magazine, which is why Homemakers Magazine is so widely read, even by me. Or, for boys, Forbes or The Economist , and Improving Your Place in the Power Hierarchy by Choosing the Right Suit. (Dark blue with a faint white pinstripe, not too far apart, in case you’re interested.)

Or maybe, I thought, I should expose glaring errors in the educational system, or compile a list of things I was taught which are palpably not true. For instance, in high school I made the mistake of taking Home Economics instead of Typing - we thought, in those days, that if you took the commercial course most of your eyebrows would come off and would have to be drawn on with a pencil for the rest of your life - where I was told that every meal should consist of a brown thing, a white thing, a yellow thing and a green thing; that it was not right to lick the spoon while cooking; and that the inside of a dress seam was as important as the outside. All three of these ideas are false and should be discarded immediately by anyone who still holds them.

Nor did anyone have the foresight to inform me that the best thing I could do for myself as a writer would be back and wrist exercises. No one has yet done a study of this, but they will, and when they start excavating and measuring the spines and arm bones of the skeletons of famous writers of the past I am sure they will find that those who wrote the longest novels, such as Dickens and Melville, also had the thickest wrists. The real reason that Emily Dickinson stuck to lyric poems with relatively few stanzas is that she had spindly fingers. You may scoff, but future research will prove me right.

But I then thought, I shouldn’t talk about writing. Few of this graduating class will wish to be writers, and those that do should by no means be encouraged. Weave a circle round them thrice, and close your eyes holy dread, because who needs the competition? What with the proliferation of Creative Writing courses, a mushroom of recent growth all but unknown in my youth, we will soon have a state of affairs in which everybody writes and nobody reads, the exact reverse of the way things were when I was composing dolorous verses in a rented cupboard on Charles Street in the early sixties.

Or maybe, I thought, I should relate to them a little known fact of shocking import, which they will remember vividly when they have all but forgotten the rest of this speech. For example: nobody ever tells you, but did you know that when you have a baby your hair falls out? Not all of it, and not all at once, but it does fall out. It has something to do with a zinc imbalance. The good news is that it does grow back in. This only applies to girls. With boys, it falls out whether you have a baby or not, and it never grows back in; but even then there is hope. In a pinch, you can resort to quotation, a commodity which a liberal arts education teaches you to treat with respect, and I offer the following: “God only made a few perfect heads, and the rest lie covered with hair.”

Which illustrates the following point: when faced with the inevitable, you always have a choice. You may not be able to alter reality, but you can alter your attitude towards it. As I learned during my liberal arts education, any symbol can have, in the imaginative context, two versions, a positive and a negative. Blood can either be the gift of life or what comes out of you when you cut your wrists in the bathtub. Or, somewhat less drastically, if you spill your milk you’re left with a glass which is either half empty or half full.

Which brings us to the hidden agenda of this speech. What you are being ejected into today is a world that is both half empty and half full. On the one hand, the biosphere is rotting away. The raindrops that keep falling on your head are also killing the fish, the trees, the animals, and, if they keep being as acid as they are now, they’ll eventually do away with things a lot closer to home, such as crops, front lawns and your digestive tract. Nature is no longer what surrounds us, we surround it, and the switch has not been for the better. On the other hand, unlike the ancient Egyptians, we as a civilization know what mistakes we are making and we also have the technology to stop making them; all that is lacking is the will.

Another example: on the one hand, we ourselves live daily with the threat of annihilation. We’re just a computer button and a few minutes away from it, and the gap between us and it is narrowing every day. We secretly think in terms not of “If the Bomb Drops” but of “When the Bomb Drops”, and it’s understandable if we sometimes let ourselves slide into a mental state of powerlessness and consequent apathy. On the other hand, the catastrophe that threatens us as a species, and most other species as well, is not unpredictable and uncontrollable, like the eruption of the volcano that destroyed Pompeii. If it occurs, we can die with the dubious satisfaction of knowing that the death of the world was a man-made and therefore preventable event, and that the failure to prevent it was a failure of human will.

This is the kind of world we find ourselves in, and it’s not pleasant. Faced with facts this depressing, the question of the economy - or how many of us in this country can afford two cars doesn’t really loom too large, but you’d never know it from reading the papers. Things are in fact a lot worse elsewhere, where expectations center not on cars and houses and jobs but on the next elusive meal. That’s part of the down side. The up side, here and now, is that this is still more or less a democracy; you don’t get shot or tortured yet for expressing an opinion, and politicians, motivated as they may be by greed and the lust for power, are nevertheless or because of this, still swayed by public opinion. The issues raised in any election are issues perceived by those who want power to be of importance to those in a position to confer it upon them. In other words, if enough people show by the issues they raise and by the way they’re willing to vote that they want changes made, then change becomes possible. You may not be able to alter reality, but you can alter your attitude towards it, and this, paradoxically, alters reality.

Try it and see.




The Art of Failure - Malcolm Gladwell
Diction Score
22.0
Complexity Grade
Incredible
Complexity Variance
196.41537618
Average Sentence Length
16
Max Length
58
Min Length
1
Structural Score
0.469230769231
Certainty Score
0.496942197672

Why some people choke and others panic.

There was a moment, in the third and deciding set of the 1993 Wimbledon final, when Jana Novotna seemed invincible. She was leading 4-1 and serving at 40-30, meaning that she was one point from winning the game, and just five points from the most coveted championship in tennis. She had just hit a backhand to her opponent, Steffi Graf, that skimmed the net and landed so abruptly on the far side of the court that Graf could only watch, in flat- footed frustration. The stands at Center Court were packed. The Duke and Duchess of Kent were in their customary place in the royal box. Novotna was in white, poised and confident, her blond hair held back with a headband–and then something happened. She served the ball straight into the net. She stopped and steadied herself for the second serve–the toss, the arch of the back–but this time it was worse. Her swing seemed halfhearted, all arm and no legs and torso. Double fault. On the next point, she was slow to react to a high shot by Graf, and badly missed on a forehand volley. At game point, she hit an overhead straight into the net. Instead of 5-1, it was now 4-2. Graf to serve: an easy victory, 4-3. Novotna to serve. She wasn’t tossing the ball high enough. Her head was down. Her movements had slowed markedly. She double-faulted once, twice, three times. Pulled wide by a Graf forehand, Novotna inexplicably hit a low, flat shot directly at Graf, instead of a high crosscourt forehand that would have given her time to get back into position: 4-4. Did she suddenly realize how terrifyingly close she was to victory? Did she remember that she had never won a major tournament before? Did she look across the net and see Steffi Graf–Steffi Graf!–the greatest player of her generation?

On the baseline, awaiting Graf’s serve, Novotna was now visibly agitated, rocking back and forth, jumping up and down. She talked to herself under her breath. Her eyes darted around the court. Graf took the game at love; Novotna, moving as if in slow motion, did not win a single point: 5-4, Graf. On the sidelines, Novotna wiped her racquet and her face with a towel, and then each finger individually. It was her turn to serve. She missed a routine volley wide, shook her head, talked to herself. She missed her first serve, made the second, then, in the resulting rally, mis-hit a backhand so badly that it sailed off her racquet as if launched into flight. Novotna was unrecognizable, not an élite tennis player but a beginner again. She was crumbling under pressure, but exactly why was as baffling to her as it was to all those looking on. Isn’t pressure supposed to bring out the best in us? We try harder. We concentrate harder. We get a boost of adrenaline. We care more about how well we perform. So what was happening to her?

At championship point, Novotna hit a low, cautious, and shallow lob to Graf. Graf answered with an unreturnable overhead smash, and, mercifully, it was over. Stunned, Novotna moved to the net. Graf kissed her twice. At the awards ceremony, the Duchess of Kent handed Novotna the runner-up’s trophy, a small silver plate, and whispered something in her ear, and what Novotna had done finally caught up with her. There she was, sweaty and exhausted, looming over the delicate white-haired Duchess in her pearl necklace. The Duchess reached up and pulled her head down onto her shoulder, and Novotna started to sob.

Human beings sometimes falter under pressure. Pilots crash and divers drown. Under the glare of competition, basketball players cannot find the basket and golfers cannot find the pin. When that happens, we say variously that people have “panicked” or, to use the sports colloquialism, “choked.” But what do those words mean? Both are pejoratives. To choke or panic is considered to be as bad as to quit. But are all forms of failure equal? And what do the forms in which we fail say about who we are and how we think?We live in an age obsessed with success, with documenting the myriad ways by which talented people overcome challenges and obstacles. There is as much to be learned, though, from documenting the myriad ways in which talented people sometimes fail.

“Choking” sounds like a vague and all-encompassing term, yet it describes a very specific kind of failure. For example, psychologists often use a primitive video game to test motor skills. They’ll sit you in front of a computer with a screen that shows four boxes in a row, and a keyboard that has four corresponding buttons in a row. One at a time, x’s start to appear in the boxes on the screen, and you are told that every time this happens you are to push the key corresponding to the box. According to Daniel Willingham, a psychologist at the University of Virginia, if you’re told ahead of time about the pattern in which those x’s will appear, your reaction time in hitting the right key will improve dramatically. You’ll play the game very carefully for a few rounds, until you’ve learned the sequence, and then you’ll get faster and faster. Willingham calls this “explicit learning.” But suppose you’re not told that the x’s appear in a regular sequence, and even after playing the game for a while you’re not aware that there is a pattern. You’ll still get faster: you’ll learn the sequence unconsciously. Willingham calls that “implicit learning”–learning that takes place outside of awareness. These two learning systems are quite separate, based in different parts of the brain. Willingham says that when you are first taught something–say, how to hit a backhand or an overhead forehand–you think it through in a very deliberate, mechanical manner. But as you get better the implicit system takes over: you start to hit a backhand fluidly, without thinking. The basal ganglia, where implicit learning partially resides, are concerned with force and timing, and when that system kicks in you begin to develop touch and accuracy, the ability to hit a drop shot or place a serve at a hundred miles per hour. “This is something that is going to happen gradually,” Willingham says. “You hit several thousand forehands, after a while you may still be attending to it. But not very much. In the end, you don’t really notice what your hand is doing at all.”

Under conditions of stress, however, the explicit system sometimes takes over. That’s what it means to choke. When Jana Novotna faltered at Wimbledon, it was because she began thinking about her shots again. She lost her fluidity, her touch. She double-faulted on her serves and mis-hit her overheads, the shots that demand the greatest sensitivity in force and timing. She seemed like a different person–playing with the slow, cautious deliberation of a beginner–because, in a sense, she was a beginner again: she was relying on a learning system that she hadn’t used to hit serves and overhead forehands and volleys since she was first taught tennis, as a child. The same thing has happened to Chuck Knoblauch, the New York Yankees’ second baseman, who inexplicably has had trouble throwing the ball to first base. Under the stress of playing in front of forty thousand fans at Yankee Stadium, Knoblauch finds himself reverting to explicit mode, throwing like a Little Leaguer again.

Panic is something else altogether. Consider the following account of a scuba-diving accident, recounted to me by Ephimia Morphew, a human-factors specialist at nasa: “It was an open-water certification dive, Monterey Bay, California, about ten years ago. I was nineteen. I’d been diving for two weeks. This was my first time in the open ocean without the instructor. Just my buddy and I. We had to go about forty feet down, to the bottom of the ocean, and do an exercise where we took our regulators out of our mouth, picked up a spare one that we had on our vest, and practiced breathing out of the spare. My buddy did hers. Then it was my turn. I removed my regulator. I lifted up my secondary regulator. I put it in my mouth, exhaled, to clear the lines, and then I inhaled, and, to my surprise, it was water. I inhaled water. Then the hose that connected that mouthpiece to my tank, my air source, came unlatched and air from the hose came exploding into my face.

“Right away, my hand reached out for my partner’s air supply, as if I was going to rip it out. It was without thought. It was a physiological response. My eyes are seeing my hand do something irresponsible. I’m fighting with myself. Don’t do it. Then I searched my mind for what I could do. And nothing came to mind. All I could remember was one thing: If you can’t take care of yourself, let your buddy take care of you. I let my hand fall back to my side, and I just stood there.”

This is a textbook example of panic. In that moment, Morphew stopped thinking. She forgot that she had another source of air, one that worked perfectly well and that, moments before, she had taken out of her mouth. She forgot that her partner had a working air supply as well, which could easily be shared, and she forgot that grabbing her partner’s regulator would imperil both of them. All she had was her most basic instinct: get air. Stress wipes out short-term memory. People with lots of experience tend not to panic, because when the stress suppresses their short- term memory they still have some residue of experience to draw on. But what did a novice like Morphew have? I searched my mind for what I could do. And nothing came to mind.

Panic also causes what psychologists call perceptual narrowing. In one study, from the early seventies, a group of subjects were asked to perform a visual acuity task while undergoing what they thought was a sixty-foot dive in a pressure chamber. At the same time, they were asked to push a button whenever they saw a small light flash on and off in their peripheral vision. The subjects in the pressure chamber had much higher heart rates than the control group, indicating that they were under stress. That stress didn’t affect their accuracy at the visual-acuity task, but they were only half as good as the control group at picking up the peripheral light. “You tend to focus or obsess on one thing,” Morphew says. “There’s a famous airplane example, where the landing light went off, and the pilots had no way of knowing if the landing gear was down. The pilots were so focussed on that light that no one noticed the autopilot had been disengaged, and they crashed the plane.” Morphew reached for her buddy’s air supply because it was the only air supply she could see.

Panic, in this sense, is the opposite of choking. Choking is about thinking too much. Panic is about thinking too little. Choking is about loss of instinct. Panic is reversion to instinct. They may look the same, but they are worlds apart.

Why does this distinction matter? In some instances, it doesn’t much. If you lose a close tennis match, it’s of little moment whether you choked or panicked; either way, you lost. But there are clearly cases when how failure happens is central to understanding why failure happens.

Take the plane crash in which John F. Kennedy, Jr., was killed last summer. The details of the flight are well known. On a Friday evening last July, Kennedy took off with his wife and sister-in-law for Martha’s Vineyard. The night was hazy, and Kennedy flew along the Connecticut coastline, using the trail of lights below him as a guide. At Westerly, Rhode Island, he left the shoreline, heading straight out over Rhode Island Sound, and at that point, apparently disoriented by the darkness and haze, he began a series of curious maneuvers: He banked his plane to the right, farther out into the ocean, and then to the left. He climbed and descended. He sped up and slowed down. Just a few miles from his destination, Kennedy lost control of the plane, and it crashed into the ocean.

Kennedy’s mistake, in technical terms, was that he failed to keep his wings level. That was critical, because when a plane banks to one side it begins to turn and its wings lose some of their vertical lift. Left unchecked, this process accelerates. The angle of the bank increases, the turn gets sharper and sharper, and the plane starts to dive toward the ground in an ever-narrowing corkscrew. Pilots call this the graveyard spiral. And why didn’t Kennedy stop the dive? Because, in times of low visibility and high stress, keeping your wings level–indeed, even knowing whether you are in a graveyard spiral–turns out to be surprisingly difficult. Kennedy failed under pressure.

Had Kennedy been flying during the day or with a clear moon, he would have been fine. If you are the pilot, looking straight ahead from the cockpit, the angle of your wings will be obvious from the straight line of the horizon in front of you. But when it’s dark outside the horizon disappears. There is no external measure of the plane’s bank. On the ground, we know whether we are level even when it’s dark, because of the motion-sensing mechanisms in the inner ear. In a spiral dive, though, the effect of the plane’s G-force on the inner ear means that the pilot feels perfectly level even if his plane is not. Similarly, when you are in a jetliner that is banking at thirty degrees after takeoff, the book on your neighbor’s lap does not slide into your lap, nor will a pen on the floor roll toward the “down” side of the plane. The physics of flying is such that an airplane in the midst of a turn always feels perfectly level to someone inside the cabin.

This is a difficult notion, and to understand it I went flying with William Langewiesche, the author of a superb book on flying, “Inside the Sky.” We met at San Jose Airport, in the jet center where the Silicon Valley billionaires keep their private planes. Langewiesche is a rugged man in his forties, deeply tanned, and handsome in the way that pilots (at least since the movie “The Right Stuff”) are supposed to be. We took off at dusk, heading out toward Monterey Bay, until we had left the lights of the coast behind and night had erased the horizon. Langewiesche let the plane bank gently to the left. He took his hands off the stick. The sky told me nothing now, so I concentrated on the instruments. The nose of the plane was dropping. The gyroscope told me that we were banking, first fifteen, then thirty, then forty-five degrees. “We’re in a spiral dive,” Langewiesche said calmly. Our airspeed was steadily accelerating, from a hundred and eighty to a hundred and ninety to two hundred knots. The needle on the altimeter was moving down. The plane was dropping like a stone, at three thousand feet per minute. I could hear, faintly, a slight increase in the hum of the engine, and the wind noise as we picked up speed. But if Langewiesche and I had been talking I would have caught none of that. Had the cabin been unpressurized, my ears might have popped, particularly as we went into the steep part of the dive. But beyond that? Nothing at all. In a spiral dive, the G-load–the force of inertia–is normal. As Langewiesche puts it, the plane likes to spiral-dive. The total time elapsed since we started diving was no more than six or seven seconds. Suddenly, Langewiesche straightened the wings and pulled back on the stick to get the nose of the plane up, breaking out of the dive. Only now did I feel the full force of the G-load, pushing me back in my seat. “You feel no G-load in a bank,” Langewiesche said. “There’s nothing more confusing for the uninitiated.”

I asked Langewiesche how much longer we could have fallen. “Within five seconds, we would have exceeded the limits of the airplane,” he replied, by which he meant that the force of trying to pull out of the dive would have broken the plane into pieces. I looked away from the instruments and asked Langewiesche to spiral-dive again, this time without telling me. I sat and waited. I was about to tell Langewiesche that he could start diving anytime, when, suddenly, I was thrown back in my chair. “We just lost a thousand feet,” he said.

This inability to sense, experientially, what your plane is doing is what makes night flying so stressful. And this was the stress that Kennedy must have felt when he turned out across the water at Westerly, leaving the guiding lights of the Connecticut coastline behind him. A pilot who flew into Nantucket that night told the National Transportation Safety Board that when he descended over Martha’s Vineyard he looked down and there was “nothing to see. There was no horizon and no light…. I thought the island might [have] suffered a power failure.” Kennedy was now blind, in every sense, and he must have known the danger he was in. He had very little experience in flying strictly by instruments. Most of the time when he had flown up to the Vineyard the horizon or lights had still been visible. That strange, final sequence of maneuvers was Kennedy’s frantic search for a clearing in the haze. He was trying to pick up the lights of Martha’s Vineyard, to restore the lost horizon. Between the lines of the National Transportation Safety Board’s report on the crash, you can almost feel his desperation:

About 2138 the target began a right turn in a southerly direction. About 30 seconds later, the target stopped its descent at 2200 feet and began a climb that lasted another 30 seconds. During this period of time, the target stopped the turn, and the airspeed decreased to about 153 KIAS. About 2139, the target leveled off at 2500 feet and flew in a southeasterly direction. About 50 seconds later, the target entered a left turn and climbed to 2600 feet. As the target continued in the left turn, it began a descent that reached a rate of about 900 fpm.

But was he choking or panicking? Here the distinction between those two states is critical. Had he choked, he would have reverted to the mode of explicit learning. His movements in the cockpit would have become markedly slower and less fluid. He would have gone back to the mechanical, self-conscious application of the lessons he had first received as a pilot–and that might have been a good thing. Kennedy needed to think, to concentrate on his instruments, to break away from the instinctive flying that served him when he had a visible horizon.

But instead, from all appearances, he panicked. At the moment when he needed to remember the lessons he had been taught about instrument flying, his mind–like Morphew’s when she was underwater–must have gone blank. Instead of reviewing the instruments, he seems to have been focussed on one question: Where are the lights of Martha’s Vineyard? His gyroscope and his other instruments may well have become as invisible as the peripheral lights in the underwater-panic experiments. He had fallen back on his instincts–on the way the plane felt–and in the dark, of course, instinct can tell you nothing. The N.T.S.B. report says that the last time the Piper’s wings were level was seven seconds past 9:40, and the plane hit the water at about 9:41, so the critical period here was less than sixty seconds. At twenty-five seconds past the minute, the plane was tilted at an angle greater than forty-five degrees. Inside the cockpit it would have felt normal. At some point, Kennedy must have heard the rising wind outside, or the roar of the engine as it picked up speed. Again, relying on instinct, he might have pulled back on the stick, trying to raise the nose of the plane. But pulling back on the stick without first levelling the wings only makes the spiral tighter and the problem worse. It’s also possible that Kennedy did nothing at all, and that he was frozen at the controls, still frantically searching for the lights of the Vineyard, when his plane hit the water. Sometimes pilots don’t even try to make it out of a spiral dive. Langewiesche calls that “one G all the way down.”

What happened to Kennedy that night illustrates a second major difference between panicking and choking. Panicking is conventional failure, of the sort we tacitly understand. Kennedy panicked because he didn’t know enough about instrument flying. If he’d had another year in the air, he might not have panicked, and that fits with what we believe–that performance ought to improve with experience, and that pressure is an obstacle that the diligent can overcome. But choking makes little intuitive sense. Novotna’s problem wasn’t lack of diligence; she was as superbly conditioned and schooled as anyone on the tennis tour. And what did experience do for her? In 1995, in the third round of the French Open, Novotna choked even more spectacularly than she had against Graf, losing to Chanda Rubin after surrendering a 5-0 lead in the third set. There seems little doubt that part of the reason for her collapse against Rubin was her collapse against Graf–that the second failure built on the first, making it possible for her to be up 5-0 in the third set and yet entertain the thought I can still lose. If panicking is conventional failure, choking is paradoxical failure.

Claude Steele, a psychologist at Stanford University, and his colleagues have done a number of experiments in recent years looking at how certain groups perform under pressure, and their findings go to the heart of what is so strange about choking. Steele and Joshua Aronson found that when they gave a group of Stanford undergraduates a standardized test and told them that it was a measure of their intellectual ability, the white students did much better than their black counterparts. But when the same test was presented simply as an abstract laboratory tool, with no relevance to ability, the scores of blacks and whites were virtually identical. Steele and Aronson attribute this disparity to what they call “stereotype threat”: when black students are put into a situation where they are directly confronted with a stereotype about their group–in this case, one having to do with intelligence–the resulting pressure causes their performance to suffer.

Steele and others have found stereotype threat at work in any situation where groups are depicted in negative ways. Give a group of qualified women a math test and tell them it will measure their quantitative ability and they’ll do much worse than equally skilled men will; present the same test simply as a research tool and they’ll do just as well as the men. Or consider a handful of experiments conducted by one of Steele’s former graduate students, Julio Garcia, a professor at Tufts University. Garcia gathered together a group of white, athletic students and had a white instructor lead them through a series of physical tests: to jump as high as they could, to do a standing broad jump, and to see how many pushups they could do in twenty seconds. The instructor then asked them to do the tests a second time, and, as you’d expect, Garcia found that the students did a little better on each of the tasks the second time around. Then Garcia ran a second group of students through the tests, this time replacing the instructor between the first and second trials with an African-American. Now the white students ceased to improve on their vertical leaps. He did the experiment again, only this time he replaced the white instructor with a black instructor who was much taller and heavier than the previous black instructor. In this trial, the white students actually jumped less high than they had the first time around. Their performance on the pushups, though, was unchanged in each of the conditions. There is no stereotype, after all, that suggests that whites can’t do as many pushups as blacks. The task that was affected was the vertical leap, because of what our culture says: white men can’t jump.

It doesn’t come as news, of course, that black students aren’t as good at test-taking as white students, or that white students aren’t as good at jumping as black students. The problem is that we’ve always assumed that this kind of failure under pressure is panic. What is it we tell underperforming athletes and students? The same thing we tell novice pilots or scuba divers: to work harder, to buckle down, to take the tests of their ability more seriously. But Steele says that when you look at the way black or female students perform under stereotype threat you don’t see the wild guessing of a panicked test taker. “What you tend to see is carefulness and second-guessing,” he explains. “When you go and interview them, you have the sense that when they are in the stereotype-threat condition they say to themselves, ‘Look, I’m going to be careful here. I’m not going to mess things up.’ Then, after having decided to take that strategy, they calm down and go through the test. But that’s not the way to succeed on a standardized test. The more you do that, the more you will get away from the intuitions that help you, the quick processing. They think they did well, and they are trying to do well. But they are not.” This is choking, not panicking. Garcia’s athletes and Steele’s students are like Novotna, not Kennedy. They failed because they were good at what they did: only those who care about how well they perform ever feel the pressure of stereotype threat. The usual prescription for failure–to work harder and take the test more seriously–would only make their problems worse.

That is a hard lesson to grasp, but harder still is the fact that choking requires us to concern ourselves less with the performer and more with the situation in which the performance occurs. Novotna herself could do nothing to prevent her collapse against Graf. The only thing that could have saved her is if–at that critical moment in the third set–the television cameras had been turned off, the Duke and Duchess had gone home, and the spectators had been told to wait outside. In sports, of course, you can’t do that. Choking is a central part of the drama of athletic competition, because the spectators have to be there–and the ability to overcome the pressure of the spectators is part of what it means to be a champion. But the same ruthless inflexibility need not govern the rest of our lives. We have to learn that sometimes a poor performance reflects not the innate ability of the performer but the complexion of the audience; and that sometimes a poor test score is the sign not of a poor student but of a good one.

Through the first three rounds of the 1996 Masters golf tournament, Greg Norman held a seemingly insurmountable lead over his nearest rival, the Englishman Nick Faldo. He was the best player in the world. His nickname was the Shark. He didn’t saunter down the fairways; he stalked the course, blond and broad-shouldered, his caddy behind him, struggling to keep up. But then came the ninth hole on the tournament’s final day. Norman was paired with Faldo, and the two hit their first shots well. They were now facing the green. In front of the pin, there was a steep slope, so that any ball hit short would come rolling back down the hill into oblivion. Faldo shot first, and the ball landed safely long, well past the cup.

Norman was next. He stood over the ball. “The one thing you guard against here is short,” the announcer said, stating the obvious. Norman swung and then froze, his club in midair, following the ball in flight. It was short. Norman watched, stone-faced, as the ball rolled thirty yards back down the hill, and with that error something inside of him broke.

At the tenth hole, he hooked the ball to the left, hit his third shot well past the cup, and missed a makable putt. At eleven, Norman had a three-and-a-half-foot putt for par–the kind he had been making all week. He shook out his hands and legs before grasping the club, trying to relax. He missed: his third straight bogey. At twelve, Norman hit the ball straight into the water. At thirteen, he hit it into a patch of pine needles. At sixteen, his movements were so mechanical and out of synch that, when he swung, his hips spun out ahead of his body and the ball sailed into another pond. At that, he took his club and made a frustrated scythelike motion through the grass, because what had been obvious for twenty minutes was now official: he had fumbled away the chance of a lifetime.

Faldo had begun the day six strokes behind Norman. By the time the two started their slow walk to the eighteenth hole, through the throng of spectators, Faldo had a four- stroke lead. But he took those final steps quietly, giving only the smallest of nods, keeping his head low. He understood what had happened on the greens and fairways that day. And he was bound by the particular etiquette of choking, the understanding that what he had earned was something less than a victory and what Norman had suffered was something less than a defeat.

When it was all over, Faldo wrapped his arms around Norman. “I don’t know what to say–I just want to give you a hug,” he whispered, and then he said the only thing you can say to a choker: “I feel horrible about what happened. I’m so sorry.” With that, the two men began to cry.